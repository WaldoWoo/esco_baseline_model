{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1STR4oRDEKHtBQn7jSueEErAfH5JdSCAw","timestamp":1733613272191}],"machine_shape":"hm","gpuType":"A100","mount_file_id":"1R9gFhg_133EVj_MG_PiAiY58ZudsX-ZH","authorship_tag":"ABX9TyNbr1xIZxDtET4+N2UdIZNP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"dBwCg3ybZf6a"},"outputs":[],"source":["# Import Libraries\n","\n","import gc\n","import pickle\n","import numpy as np\n","import pandas as pd\n","import lightgbm as lgb\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import root_mean_squared_error, mean_squared_error\n","from sklearn.metrics import mean_absolute_percentage_error\n","from sklearn.model_selection import GridSearchCV\n","import os\n","\n"]},{"cell_type":"code","source":["\n","# Retrieve dataset and split into test and training sets\n","root = \"/content/drive/MyDrive/DTSA 5511 - Deep Learning/esco_predict\"\n","dir = os.path.join(root, \"processed_data\")\n","file = os.path.join(dir, \"processed_training_set.pkl\")\n","train_data = pd.read_pickle(file)"],"metadata":{"id":"KafrT6NMZnMA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Separate features and traget variable\n","target = \"usage_kWh\"\n","X_train = train_data.drop(columns=[target])  # Drop target\n","y_train = train_data[target]"],"metadata":{"id":"_O3BehWwPqoM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create custom scorer for RMSLE\n","# Good article on RMSLE: https://medium.com/analytics-vidhya/root-mean-square-log-error-rmse-vs-rmlse-935c6cc1802a\n","\n","def rmsle(y_true, y_pred):\n","  y_pred = np.clip(y_pred, 0, None) #Any value in y_pred that is less than 0 will be replaced with 0\n","  return np.sqrt(mean_squared_error(np.log1p(y_true), np.log1p(y_pred)))"],"metadata":{"id":"yNuU7z8uGOYM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n","from lightgbm import LGBMRegressor\n","from sklearn.metrics import make_scorer\n","\n","def gridsearch_lgbm(X_train, y_train, param_grid):\n","\n","\n","  # Initialize TimeSeriesSplit\n","  tscv = TimeSeriesSplit(n_splits=5)\n","\n","  rmsle_scorer = make_scorer(rmsle, greater_is_better=False)\n","  # mape = mean_absolute_percentage_error(y_test, y_pred)\n","\n","  # Initialize the LGBM Regressor\n","  lgbm = LGBMRegressor(objective=\"regression\", boosting_type=\"gbdt\", metric=\"rmse\")\n","\n","  # Initialize GridSearchCV\n","  grid_search = GridSearchCV(\n","      estimator=lgbm,\n","      param_grid=param_grid,\n","      scoring=rmsle_scorer,\n","      cv=tscv,\n","      # verbose=0\n","  )\n","\n","  # Perform grid search\n","  grid_search.fit(X_train, y_train)\n","\n","  # Get the best parameters and the best score\n","  best_params = grid_search.best_params_\n","  best_score = -grid_search.best_score_  # Negate because RMSE scorer is reversed\n","  # print(\"Best Parameters:\", best_params)\n","  print(f\"Best RMSLE: {best_score}\")\n","  return best_params\n","\n"],"metadata":{"id":"ePINwFdb4Z19"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Free memory\n","# del train_df\n","# del test_df\n","# gc.collect() #Release memory\n","# print(best_params)\n","\n","# Define LightGBM Parameters\n","def feature_importance(best_params, gain_tol, train_data, X_train, y_train):\n","    # feature_importance(best_params, gain_tol, X_train[imp_features], y_train)\n","  # best_params = {'learning_rate': 0.01, 'max_depth': 5, 'min_data_in_leaf': 50, 'num_leaves': 31, 'verbosity': -1}\n","  parameters = {\n","      \"objective\": \"regression\",\n","      \"boosting_type\": \"gbdt\",\n","      \"seed\": 42,\n","      \"verbosity\": -1,  # Suppress warnings and logs\n","  }\n","\n","  parameters.update(best_params)\n","\n","  # Train the model\n","  train_data = lgb.Dataset(X_train, label = y_train, params={\"feature_pre_filter\": False})\n","  lgb_model = lgb.train(parameters, train_data)\n","\n","  # Make predictions on the test set\n","  y_pred = lgb_model.predict(X_train)\n","\n","  importance = lgb_model.feature_importance(importance_type = \"gain\")\n","  feature_names = X_train.columns\n","  feature_importance_df = pd.DataFrame({\"Feature\": feature_names, \"Importance\": importance})\n","  feature_importance_df = feature_importance_df.sort_values(by=\"Importance\", ascending=False)\n","  feature_importance_df[\"% Total Gain\"] = round(feature_importance_df[\"Importance\"] / feature_importance_df[\"Importance\"].sum()*100, 2)\n","  print(feature_importance_df,\"\\n\")\n","\n","  imp_features_df = feature_importance_df[feature_importance_df[\"% Total Gain\"] > 1.0]\n","  imp_features = imp_features_df[\"Feature\"].tolist()\n","  # print(imp_features_df)\n","  X_train = X_train[imp_features]\n","  return X_train, imp_features_df"],"metadata":{"id":"QYXVxhzBKK4K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def RFECV_lgbm(X_train, y_train, param_grid, gain_tol=0.01):\n","    \"\"\"\n","    Recursive Feature Elimination with Cross-Validation for LightGBM\n","\n","    Args:\n","        X_train (pd.DataFrame): Feature set for training\n","        y_train (pd.Series): Target variable for training\n","        param_grid (dict): Parameter grid for LightGBM model tuning\n","        gain_tol (float): Minimum percentage of total gain required for a feature to remain\n","\n","    Returns:\n","        tuple: A tuple containing the important features and the best parameters\n","    \"\"\"\n","    imp_features = X_train.columns.tolist()  # Start with all features\n","    best_params = {}\n","    iteration = 1\n","\n","    while True:\n","        print(f\"\\nIteration {iteration}...\")\n","        print(f\"Using features: {imp_features}\\n\")\n","\n","        # Grid search to find the best parameters\n","        best_params = gridsearch_lgbm(X_train[imp_features], y_train, param_grid)\n","\n","        # Train the model and compute feature importance\n","        _, feature_importance_df = feature_importance(\n","            best_params, gain_tol, None, X_train[imp_features], y_train)\n","\n","        # Identify features above the gain tolerance threshold\n","        current_imp_features = feature_importance_df[\n","            feature_importance_df[\"% Total Gain\"] >= gain_tol * 100][\"Feature\"].tolist()\n","\n","        print(f\"Selected {len(current_imp_features)} features after filtering with gain_tol = {gain_tol * 100:.2f}%.\")\n","\n","        # Stop if no features were removed or only one feature is left\n","        if len(current_imp_features) == len(imp_features) or len(current_imp_features) <= 1:\n","            print(f\"Stopping after iteration {iteration}. Remaining features: {current_imp_features}\")\n","            break\n","\n","        # Update features for the next iteration\n","        imp_features = current_imp_features\n","        iteration += 1\n","\n","    return imp_features, best_params\n"],"metadata":{"id":"mEYcDSflXZ8h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Parameter Descriptions for LightGBM\n","\n","**learning_rate**:  Controls the step size for updating weights in each boosting iteration. Lower values make training slower but can improve accuracy by refining each step.\n","\n","**num_iterations**:  *This code doesn't support the cross-validation of num_iterations. Uses default of 100.*  \n","The maximum number of boosting iterations (trees) to train. Higher values allow the model to learn more but increase training time.\n","\n","**num_leaves**:  The maximum number of leaves in a single tree. A higher value allows more complex tree structures but increases the risk of overfitting.\n","\n","**max_depth**:  The maximum depth of each tree. `-1` means no limit on tree depth. Constraining depth can reduce overfitting but may underfit the data if too shallow.\n","\n","**min_data_in_leaf**:  Minimum number of samples required in a leaf node. Helps control overfitting; higher values create more generalized trees.\n","\n","**verbosity**:  Controls the amount of output during training. `-1` means silent mode, useful for suppressing logs in automated workflows.\n"],"metadata":{"id":"bzBovXe_M_ob"}},{"cell_type":"code","source":["#Define the parameter grid\n","# param_grid = {\n","#     \"learning_rate\": [0.01, 0.05, 0.1],\n","#     \"num_leaves\": [2, 5, 15, 20],\n","#     \"max_depth\": [2, 5, 10],\n","#     \"min_data_in_leaf\": [10, 20, 50, 100],\n","#     \"metric\": ['rmse', 'mae', 'mape'],\n","#     \"verbosity\": [-1]\n","# }\n","\n","param_grid = {'learning_rate': [0.05], 'max_depth': [5], 'metric': ['rmse'], 'min_data_in_leaf': [50], 'num_leaves': [20], 'verbosity': [-1]}\n","\n","\n","imp_features, best_params = RFECV_lgbm(X_train, y_train, param_grid, gain_tol=0.01)"],"metadata":{"id":"ciKuWAlJhztJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Train the Model with Optimal Parameters\n","Also export the optimal parameters and features"],"metadata":{"id":"yLrd9MAhTYVV"}},{"cell_type":"code","source":["train_set = lgb.Dataset(X_train[imp_features], label = y_train)\n","final_model = lgb.train(params = best_params, train_set = train_set)\n","\n","# Save the trained model to a .pkl file\n","import pickle\n","output_path = \"/content/drive/MyDrive/DTSA 5511 - Deep Learning/esco_predict/trained_model/trained_model.pkl\"\n","with open(output_path, \"wb\") as file:\n","    pickle.dump(final_model, file)"],"metadata":{"id":"WSibAooRT1Ky"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(imp_features)\n","print(best_params)\n","\n","# Save both variables in a .pkl file\n","output_path = \"/content/drive/MyDrive/DTSA 5511 - Deep Learning/esco_predict/trained_model/model_metadata.pkl\"\n","with open(output_path, \"wb\") as pkl_file:\n","    pickle.dump({\"imp_features\": imp_features, \"best_params\": best_params}, pkl_file)"],"metadata":{"id":"o_oV66CRTMie"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import xgboost as xgb\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","import numpy as np\n","\n","# Assuming your dataset is a Pandas DataFrame called `dataset`\n","# and your target variable is stored in the column 'usage_kWh'\n","\n","# Define the features and target\n","X = X_train[imp_features]\n","y = y_train\n","\n","\n","# XGBoost parameters similar to the LGBM params\n","xgb_params = {\n","    'learning_rate': 0.05,\n","    'max_depth': 5,\n","    'objective': 'reg:squarederror',\n","    'n_estimators': 1000,  # Can be adjusted\n","    'min_child_weight': 50,  # Similar to min_data_in_leaf\n","    'colsample_bytree': 1.0,\n","    'subsample': 1.0,\n","    'verbosity': 0,\n","}\n","\n","# Initialize the model\n","xgb_model = xgb.XGBRegressor(**xgb_params)\n","\n","# Train the model without cross-validation or evaluation\n","xgb_model.fit(X_train, y_train, verbose=False)\n","\n","\n","output_path = \"/content/drive/MyDrive/DTSA 5511 - Deep Learning/esco_predict/trained_model/xg_trained_model.pkl\"\n","with open(output_path, \"wb\") as file:\n","    pickle.dump(final_model, file)"],"metadata":{"id":"KaqvRRUsr2wG"},"execution_count":null,"outputs":[]}]}